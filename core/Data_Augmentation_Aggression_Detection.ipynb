{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Augmentation_Aggression_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGsrg3dud/HLo71fLUZoQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dutta-SD/AggDetect/blob/master/core/Data_Augmentation_Aggression_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJel8H9qjsC"
      },
      "source": [
        "# Data Augmentation for Aggression and Misogyny Detection using BERT\n",
        "# Â© Sandip Dutta, 2021\n",
        "---\n",
        "Since we did not own a GPU, so we trained the BERT data augmentation pipeline in Google Colab.\n",
        "We downloaded the `.csv` files from colab and used it for our purpose.\n",
        "\n",
        "We do not refactor the code into `.py` files as we would not be able to test without a GPU.\n",
        "Therefore, IPython NoteBook format is retained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpBexot_igNT"
      },
      "source": [
        "# Downloads and Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY1DlOpGW4tv"
      },
      "source": [
        "# ENABLE GPU BEFORE PROCEEDING WITH NOTEBOOK\n",
        "# Transformers library -- contains BERT based models\n",
        "! pip3 install -qq transformers\n",
        "# Import Libraries\n",
        "import transformers\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiwgPXymi5Qg"
      },
      "source": [
        "# Data Specific Parameters\n",
        "The data has two target columns\n",
        "\n",
        "*   Sub Task 1 - Aggression Classification into 3 classes\n",
        "*   Sub Task 2 - Misogyny Detection into 2 classes\n",
        "\n",
        "We keep one task for augmentation and delete another task for one run. \n",
        "Then we repeat for the next task by changing `non_aug_map` to `aug_map` and vice versa.\n",
        "\n",
        "We do this to prevent colab environment from crashing due to limited size of available resources.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvby_T0JCgqP",
        "outputId": "50ff7e7f-ab78-4247-b6f5-9b4bdb3ec0b4"
      },
      "source": [
        "!pip freeze | grep transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transformers==4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMowxnX54GuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8754354-645c-4b77-d6d9-6fe3d77701de"
      },
      "source": [
        "# nltk setup\n",
        "nltk.download('stopwords')\n",
        "# Set random seed\n",
        "RANDOM_SEED = 0\n",
        "transformers.trainer_utils.set_seed(RANDOM_SEED)\n",
        "\n",
        "# Number of times to augment\n",
        "NUM_TIMES_TO_AUGMENT = 2\n",
        "\n",
        "# Stop words for filtering\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Train Data URL\n",
        "train_data_url = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/trac2_eng_train.csv'\n",
        "\n",
        "# Map of tasks\n",
        "T1 = {\n",
        "    'task' : 'Sub-task B',\n",
        "    'map' : {\n",
        "        'NGEN' : 0,\n",
        "        'GEN' : 1,\n",
        "    },\n",
        "    'low' : ('GEN',)\n",
        "}\n",
        "\n",
        "T2 = {\n",
        "    'task' : 'Sub-task A',\n",
        "    'map' : {\n",
        "        'NAG' : 0,\n",
        "        'CAG' : 1,\n",
        "        'OAG' : 2,\n",
        "    },\n",
        "    'low' : ('CAG', 'OAG')\n",
        "}\n",
        "\n",
        "# Set the values\n",
        "# aug_map : task to be augmented\n",
        "# non_aug_map : task to be discarded\n",
        "aug_map, non_aug_map = T1, T2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSDWCveUk2n_"
      },
      "source": [
        "# Fetch the Data and Remove one Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKcY1ylxE6V"
      },
      "source": [
        "# Fetch the data and remove irrelevant columns\n",
        "train = pd.read_csv(train_data_url)\n",
        "train.drop(['ID', non_aug_map['task']], axis = 1, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgij0hc1kqnk"
      },
      "source": [
        "# Preprocess and Augmentation Training\n",
        "Define functions for preprocessing and Augmentation Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xRQicfGWm3F"
      },
      "source": [
        "def create_aug_pipeline(model_name : str):\n",
        "    \"\"\"\n",
        "    Returns the Mask word filling BERT language model. This\n",
        "    is the main model that will do the augmentation for us.\n",
        "    This fills words masked with [MASK] token into the most\n",
        "    likely word. So we get additional data. Some noise might\n",
        "    be present in the data, which helps reduce overfitting.\n",
        "\n",
        "    Args:\n",
        "        model_name (string) : Denotes the model names to use for\n",
        "        augmentation. See Transformers library for more details.\n",
        "\n",
        "    Returns:\n",
        "        transformers.Pipeline object\n",
        "    \"\"\"\n",
        "    \n",
        "    return transformers.pipeline(\"fill-mask\", model_name, device=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gh_bpwsYRUb"
      },
      "source": [
        "def stringCleanerMasker(\n",
        "    ip_string : str, \n",
        "    stop_words : set, \n",
        "    num_mask_per_str : int = 1, \n",
        "    mask_delim : str = '[MASK]',\n",
        "    max_valid_length : int = 512,\n",
        "    max_mask_delim_replace : int = 1,\n",
        "    invalid_string : str = 'INVALID'\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Cleans the string, removes stopwords and masks certain words.\n",
        "    These words will be replaced with a special token. The BERT Augmentation\n",
        "    pipeline will predict these masked words and give us augmented data.\n",
        "\n",
        "    Args:\n",
        "        ip_string (str) : input string to mask\n",
        "        stop_words (set) : set of stopwords to filter from data\n",
        "        num_mask_per_str (int, Optional, default = 1) : number of tokens to mask in text\n",
        "        mask_delim (str, Optional, default = '[MASK]') : The token which will mask tokens in text\n",
        "        max_valid_length (int, Optional, default = 512) : Max length of the number of input tokens.\n",
        "        max_mask_delim_replace (int, Optional, default = 1) : max tokens to replace with mask.\n",
        "        invalid_string (str, Optional, default = 'INVALID') : Invalid string to return in case of\n",
        "        Exception\n",
        "\n",
        "    Returns:\n",
        "        str, cleaned string with masked token\n",
        "    \"\"\"   \n",
        "\n",
        "    # Remove Stop Words\n",
        "    ip_list = [tok for tok in ip_string.split() if tok not in stop_words]\n",
        "    length = len(ip_list)\n",
        "\n",
        "    if length > max_valid_length:\n",
        "        return invalid_string\n",
        "\n",
        "    try:\n",
        "        mask_token = np.random.choice(ip_list, num_mask_per_str)[0]\n",
        "        finalString = ' '.join(ip_list)\n",
        "        finalString = finalString.replace(str(mask_token), mask_delim, max_mask_delim_replace)\n",
        "        return finalString\n",
        "    except Exception as e:\n",
        "        return invalid_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AybadDfXdiJ3"
      },
      "source": [
        "def appendAugDataToDataFrame(\n",
        "    train : pd.DataFrame,\n",
        "    aug_pipe_model_name : str, \n",
        "    stopwords : set, \n",
        "    target_col : str,\n",
        "    target_label : list,\n",
        "    invalid_string : str = 'INVALID',\n",
        "    top_k : int = 2,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Created Augmented Data using augmentation pipeline.\n",
        "    Then append to list. Replace non augmented data frame\n",
        "    with augemented data.\n",
        "\n",
        "    Args:\n",
        "        train (DataFrame) : The dataframe to append data to\n",
        "        aug_pipe_model_name (str) : BERT model name string \n",
        "        stopwords (set) : set of stopwords\n",
        "        target_col (str) : name of column to augment\n",
        "        target_label (list) : target labels to augment. Labels that have less count in data is \n",
        "        mentioned here\n",
        "        invalid_string (str, default = 'INVALID') : string to detect wheter to augment text or not.\n",
        "        top_k (int, default = 2) : The number of top predictions to append per\n",
        "        masked string.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame - DataFrame containing augmented Data\n",
        "    \"\"\"\n",
        "    \n",
        "    # which data to augment\n",
        "    subset = train[target_col].isin(target_label)\n",
        "\n",
        "    _data = train[subset]\n",
        "\n",
        "    _text, _labels = _data['Text'], _data[target_col]\n",
        "\n",
        "    # Augmentation Pipeline, \n",
        "    aug_pipe = create_aug_pipeline(aug_pipe_model_name)\n",
        "    \n",
        "    # Augmented Data\n",
        "    aug_data = []\n",
        "    \n",
        "    def append_string(string, target_label, stopwords, aug_container):\n",
        "        \"\"\"Appends string to given container\"\"\"\n",
        "        clean_string = stringCleanerMasker(string, stop_words = stopwords)\n",
        "        # If get invalid string, add it\n",
        "        if clean_string == invalid_string:\n",
        "            aug_container.append((string, target_label))\n",
        "            return aug_container\n",
        "\n",
        "        # Generate augementations\n",
        "        aug_preds = aug_pipe(clean_string)\n",
        "\n",
        "        # Top 2 predictions\n",
        "        for item in aug_preds[:top_k]:\n",
        "            sentence = item['sequence']\n",
        "            aug_container.append((sentence, target_label))\n",
        "\n",
        "        return aug_container\n",
        "\n",
        "    for x, y in tqdm(zip(_text, _labels), desc = 'Augmenting... '):\n",
        "        aug_data.extend(append_string(x, y, stopwords, []))\n",
        "\n",
        "    aug_data = pd.DataFrame(aug_data, columns = train.columns)\n",
        "    df = train.append(aug_data, ignore_index = True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1nAaZPtsTP4"
      },
      "source": [
        "# Training Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxH7Dr97eBDz",
        "outputId": "e6d9a3bf-b045-47af-a456-7e069d34b086"
      },
      "source": [
        "# Augmentation for specified number  of times\n",
        "\n",
        "for _ in range(NUM_TIMES_TO_AUGMENT):\n",
        "    train = appendAugDataToDataFrame(\n",
        "        train = train,\n",
        "        aug_pipe_model_name = \"bert-base-multilingual-cased\",\n",
        "        stopwords = stop_words,\n",
        "        target_col = aug_map['task'],\n",
        "        target_label = aug_map['low']\n",
        "    )\n",
        "# Below warning is normal, ignore\n",
        "# can run twice for additional data\n",
        "# As more data accumulates time increases\n",
        "# TASK B time ~ 2 minutes for default and GPU\n",
        "# TASK A time ~ 2 minutes for default and GPU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Augmenting... : 309it [00:10, 30.53it/s]\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Augmenting... : 927it [00:31, 29.59it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJr8hYlpsdEp"
      },
      "source": [
        "# Validation that the Augmentation model worked fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXhswViRvuHz",
        "outputId": "a2fd934f-380e-4c88-8e4a-b69bc972d46b"
      },
      "source": [
        "train[aug_map['task']].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NGEN    3954\n",
              "GEN     2780\n",
              "Name: Sub-task B, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxPDUKmKCNVc"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIaKjfkwpiqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "e5f6d7db-89c7-49d1-9c92-4c6c7042fcff"
      },
      "source": [
        "# This data is added to the prediction pipeline.\n",
        "# We download it and to data folder of prediction pipeline\n",
        "file_name = f\"Final_AUG_{aug_map['task']}_ENGLISH.csv\"\n",
        "\n",
        "# For inputting to input folder, uncomment below lines\n",
        "\n",
        "\"\"\"\n",
        "# Assuming this file is in '/core' folder\n",
        "path = '../input/'\n",
        "file_name = path + file_name\n",
        "\"\"\"\n",
        "train.to_csv(file_name, index = False)\n",
        "\n",
        "# For storing to input folder, remove this line\n",
        "files.download(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_83693502-b654-40e0-819c-9cfc68690096\", \"Final_AUG_Sub-task B_ENGLISH.csv\", 752080)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}